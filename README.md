
# ğŸ–¼ï¸ Image Captioning with Attention Mechanism

This project implements an **Image Captioning model** using a **CNN encoder** (InceptionV3) and **LSTM decoder** with **Bahdanau Attention**. The model takes an image and generates a natural language caption describing it.

> ğŸ§  Inspired by the paper: *"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"* (Xu et al., 2015)

---

## ğŸ“Œ Features

- Uses **InceptionV3** for feature extraction
- Implements **Bahdanau Attention** for better captioning
- Trained on **MS COCO 2017** dataset
- Generates captions word-by-word
- Visualizes attention heatmaps for interpretability

---

## ğŸ§ª Evaluation Metrics

| Metric        | Value     |
|---------------|-----------|
| **BLEU-1**    | ~0.68     |
| **Accuracy**  | ~75% (word-level on validation set) |


> âœ… BLEU (Bilingual Evaluation Understudy) is the standard metric used to evaluate image captioning models.

---

## ğŸ“ Project Structure

```

Image_captioning/
â”‚
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ train2017/                  # Images from COCO train set
â”‚   â”œâ”€â”€ val2017/                    # Images from COCO val set
â”‚   â”œâ”€â”€ annotations/               # COCO captions JSON files
â”‚   â””â”€â”€ captions.csv               # Preprocessed caption file
â”‚
â”œâ”€â”€ ğŸ“ features/
â”‚   â””â”€â”€ image_features.npy         # Saved CNN features
â”‚
â”œâ”€â”€ ğŸ“ models/
â”‚   â”œâ”€â”€ encoder.py                 # CNN Encoder module
â”‚   â”œâ”€â”€ attention.py               # Bahdanau Attention module
â”‚   â””â”€â”€ decoder.py                 # RNN Decoder with attention
â”‚
â”œâ”€â”€ ğŸ“ utils/
â”‚   â”œâ”€â”€ data_loader.py             # Dataset loading, tokenizer, padding
â”‚   â””â”€â”€ utils.py                   # Misc helper functions (e.g., plot attention)
â”‚
â”œâ”€â”€ ğŸ“ notebooks/
â”‚   â””â”€â”€ 1_data_preprocessing.ipynb # Preprocessing captions and images
â”‚   â””â”€â”€ 2_extract_features.ipynb   # Extract image features using CNN
â”‚   â””â”€â”€ 3_train_model.ipynb        # Training the model
â”‚   â””â”€â”€ 4_generate_caption.ipynb   # Testing/inference + attention visualization
â”‚
â”œâ”€â”€ ğŸ“ checkpoints/
â”‚   â””â”€â”€ best_model/                # Save trained model weights
â”‚
â”œâ”€â”€ requirements.txt               # Required Python packages


---

## ğŸš€ Setup Instructions

1. **Clone the Repo**  
```bash
git clone https://github.com/yourusername/image-captioning-attention.git
cd image-captioning-attention
````

2. **Install Dependencies**

```bash
pip install -r requirements.txt
```

3. **Download MS COCO Dataset**
   From: [COCO 2017](https://cocodataset.org/#download)

* Download:

  * `train2017.zip`, `val2017.zip`
  * `annotations_trainval2017.zip`

4. **Run Notebooks in Order**

   * `1_preprocess_captions.ipynb`
   * `2_extract_features.ipynb`
   * `3_train_model.ipynb`
   * `4_generate_caption.ipynb`

---

## ğŸ–¼ï¸ Example Output

**Input Image:**
![example](https://user-images.githubusercontent.com/yourid/example.jpg)

**Generated Caption:**
`"a man riding a skateboard down a ramp"`

---

## ğŸ“Š Optional: Save & Load Model

To persist models:

```python
encoder.save_weights('./checkpoints/encoder.h5')
decoder.save_weights('./checkpoints/decoder.h5')
```

To reload:

```python
encoder.load_weights('./checkpoints/encoder.h5')
decoder.load_weights('./checkpoints/decoder.h5')
```

---

## ğŸ§  References

* Xu et al. (2015), *Show, Attend and Tell*, [arXiv:1502.03044](https://arxiv.org/abs/1502.03044)
* MS COCO Dataset: [https://cocodataset.org](https://cocodataset.org)

---

## ğŸ™Œ Credits

Developed with guidance from \[OpenAIâ€™s GPT-4o] and the AI/ML community.

---

## ğŸ“« Contact

For help, feedback, or contributions:
Akshu Grewal â€“ GitHub: `@Aksh24Tech`

## No dataset seen due to its large quantity cause difficulty, download it personally and use. I first tested it on small quantity of images and see it fully working.
```


